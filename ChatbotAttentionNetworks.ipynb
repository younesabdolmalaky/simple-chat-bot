{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younesabdolmalaky/simple-chat-bot/blob/main/ChatbotAttentionNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bBe8Ubw2wQB3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xF7-k4KOwQCN"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Z4ntiHFnwQCO"
      },
      "outputs": [],
      "source": [
        "def preprocess_senetence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.~,])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.~,]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    w = '<strat> ' + w + ' <end>'\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "t4k03nVIwQCa"
      },
      "outputs": [],
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ec-iU7DmwQCb"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XZ10kz9XwQCc"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    df = pd.read_csv('/content/dialogs.txt',sep='\\t' , names = ['qus' , 'ans'])\n",
        "    inp_lang = df['qus']\n",
        "    targ_lang = df['ans']\n",
        "    inp_lang = inp_lang.apply(preprocess_senetence)\n",
        "    targ_lang = targ_lang.apply(preprocess_senetence)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, target_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, target_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = '/content/dialogs.txt'"
      ],
      "metadata": {
        "id": "AMyfw4LL8Ztg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-k7Liy7gwQCe"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer = load_dataset(path_to_file, 20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DzOjVGxVwQCg"
      },
      "outputs": [],
      "source": [
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "aGsJaUoCwQCj"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Ji075IxlwQCl"
      },
      "outputs": [],
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print(t, ' .... ', lang.index_word[t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Uf-nzCpOwQCp"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(input_lang_tokenizer.word_index) + 1\n",
        "vocab_targ_size = len(target_lang_tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "oAmxOzBtwQCq"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "necMaGlEwQCq"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True)\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "    def initilize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "HkSQzmGywQCu"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sOOf5AZwQCu",
        "outputId": "fb85c516-e18b-454a-ef91-c913548741bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Encoder at 0x7f010d540df0>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "tdPcjUB8wQCv"
      },
      "outputs": [],
      "source": [
        "simple_hidden = encoder.initilize_hidden_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "gY5xRa0BwQCx"
      },
      "outputs": [],
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "x0oTHg9jwQCy"
      },
      "outputs": [],
      "source": [
        "simple_output, simple_states = encoder(example_input_batch, simple_hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "KDhKsp0WwQCz"
      },
      "outputs": [],
      "source": [
        "class Attention(keras.layers.Layer):\n",
        "    def __init__ (self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "    def call (self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "        atteion_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = atteion_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, atteion_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "woRfNnP2wQC1"
      },
      "outputs": [],
      "source": [
        "attention_layer = Attention(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JxOWcdM4wQC3"
      },
      "outputs": [],
      "source": [
        "attention_result, attention_weights = attention_layer(simple_hidden, simple_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Bcv0K8tvwQC4"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.Model):\n",
        "    def __init__ (self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True)\n",
        "        self.fc = keras.layers.Dense(vocab_size)\n",
        "        self.attention = Attention(self.dec_units)\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "v6C7Zw5VwQC5"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_targ_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pqQCZZvFwQC8"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kNqigS-_wQC8"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AUJ-4dIYwQC-"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = 'chckpnts'\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nUKY8SENwQC_"
      },
      "outputs": [],
      "source": [
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<strat>']] * BATCH_SIZE, 1)\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time "
      ],
      "metadata": {
        "id": "4fN7fCG865EQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ps4cU1bwQDA",
        "outputId": "85169a90-2f5a-4e61-88ef-fc2c64c08ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f007c356af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f007c356af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time:  9.217735767364502\n",
            "Epoch:  0\n",
            "Loss:  3.1644964\n",
            "time:  1.0533947944641113\n",
            "Epoch:  0\n",
            "Loss:  2.6362174\n",
            "time:  1.0678856372833252\n",
            "Epoch:  0\n",
            "Loss:  2.772146\n",
            "time:  1.0532371997833252\n",
            "Epoch:  0\n",
            "Loss:  2.6294649\n",
            "time:  1.0621612071990967\n",
            "Epoch:  0\n",
            "Loss:  2.8173275\n",
            "time:  0.8665311336517334\n",
            "Epoch:  0\n",
            "Loss:  2.2959116\n",
            "time:  1.0470256805419922\n",
            "Epoch:  0\n",
            "Loss:  2.315863\n",
            "time:  0.8492817878723145\n",
            "Epoch:  0\n",
            "Loss:  2.2259402\n",
            "time:  1.121849775314331\n",
            "Epoch:  0\n",
            "Loss:  2.0730305\n",
            "time:  1.4735946655273438\n",
            "Epoch:  0\n",
            "Loss:  2.1963422\n",
            "time:  1.0733022689819336\n",
            "Epoch:  0\n",
            "Loss:  2.1616414\n",
            "time:  0.662851095199585\n",
            "Epoch:  0\n",
            "Loss:  2.108451\n",
            "time:  0.8402297496795654\n",
            "Epoch:  0\n",
            "Loss:  1.9200655\n",
            "time:  1.056610107421875\n",
            "Epoch:  0\n",
            "Loss:  2.1024466\n",
            "time:  1.0729787349700928\n",
            "Epoch:  0\n",
            "Loss:  2.0974994\n",
            "time:  1.087644100189209\n",
            "Epoch:  0\n",
            "Loss:  1.953272\n",
            "time:  1.0589675903320312\n",
            "Epoch:  0\n",
            "Loss:  1.8504878\n",
            "time:  1.1586599349975586\n",
            "Epoch:  0\n",
            "Loss:  2.0269992\n",
            "time:  1.1953589916229248\n",
            "Epoch:  0\n",
            "Loss:  2.1483955\n",
            "time:  1.1709721088409424\n",
            "Epoch:  0\n",
            "Loss:  2.1444774\n",
            "time:  1.480492115020752\n",
            "Epoch:  0\n",
            "Loss:  1.8170794\n",
            "time:  0.9928863048553467\n",
            "Epoch:  0\n",
            "Loss:  2.0271466\n",
            "time:  1.0474796295166016\n",
            "Epoch:  0\n",
            "Loss:  1.9646997\n",
            "time:  0.6426053047180176\n",
            "Epoch:  0\n",
            "Loss:  1.9968385\n",
            "time:  0.8375711441040039\n",
            "Epoch:  0\n",
            "Loss:  1.8033549\n",
            "time:  0.6461954116821289\n",
            "Epoch:  0\n",
            "Loss:  1.9436854\n",
            "time:  0.6379616260528564\n",
            "Epoch:  0\n",
            "Loss:  1.7922732\n",
            "time:  1.099393367767334\n",
            "Epoch:  0\n",
            "Loss:  1.9708551\n",
            "time:  0.8310818672180176\n",
            "Epoch:  0\n",
            "Loss:  1.9421173\n",
            "time:  0.8297624588012695\n",
            "Epoch:  1\n",
            "Loss:  1.6927652\n",
            "time:  1.0575275421142578\n",
            "Epoch:  1\n",
            "Loss:  1.7280236\n",
            "time:  0.8313031196594238\n",
            "Epoch:  1\n",
            "Loss:  1.8506781\n",
            "time:  1.7629051208496094\n",
            "Epoch:  1\n",
            "Loss:  1.9611483\n",
            "time:  1.1651175022125244\n",
            "Epoch:  1\n",
            "Loss:  1.8133298\n",
            "time:  0.6559638977050781\n",
            "Epoch:  1\n",
            "Loss:  1.8612\n",
            "time:  0.6504964828491211\n",
            "Epoch:  1\n",
            "Loss:  1.8110309\n",
            "time:  0.8510682582855225\n",
            "Epoch:  1\n",
            "Loss:  1.6666368\n",
            "time:  1.0310018062591553\n",
            "Epoch:  1\n",
            "Loss:  2.0683007\n",
            "time:  0.8486361503601074\n",
            "Epoch:  1\n",
            "Loss:  2.0596435\n",
            "time:  0.730842113494873\n",
            "Epoch:  1\n",
            "Loss:  1.8403116\n",
            "time:  1.7613329887390137\n",
            "Epoch:  1\n",
            "Loss:  1.8661103\n",
            "time:  0.7855136394500732\n",
            "Epoch:  1\n",
            "Loss:  1.8200451\n",
            "time:  1.315657377243042\n",
            "Epoch:  1\n",
            "Loss:  1.7107921\n",
            "time:  1.5034477710723877\n",
            "Epoch:  1\n",
            "Loss:  1.8159627\n",
            "time:  1.9531292915344238\n",
            "Epoch:  1\n",
            "Loss:  1.9087945\n",
            "time:  2.1802310943603516\n",
            "Epoch:  1\n",
            "Loss:  1.7523156\n",
            "time:  0.9144585132598877\n",
            "Epoch:  1\n",
            "Loss:  1.6124197\n",
            "time:  1.1271178722381592\n",
            "Epoch:  1\n",
            "Loss:  1.853744\n",
            "time:  1.6209707260131836\n",
            "Epoch:  1\n",
            "Loss:  1.7907681\n",
            "time:  0.8641378879547119\n",
            "Epoch:  1\n",
            "Loss:  1.7566906\n",
            "time:  0.8465089797973633\n",
            "Epoch:  1\n",
            "Loss:  1.8175778\n",
            "time:  0.837848424911499\n",
            "Epoch:  1\n",
            "Loss:  1.8202986\n",
            "time:  0.8283376693725586\n",
            "Epoch:  1\n",
            "Loss:  1.6971222\n",
            "time:  0.6227116584777832\n",
            "Epoch:  1\n",
            "Loss:  1.828041\n",
            "time:  0.9992880821228027\n",
            "Epoch:  1\n",
            "Loss:  2.0027082\n",
            "time:  0.8590433597564697\n",
            "Epoch:  1\n",
            "Loss:  1.8549742\n",
            "time:  0.9019143581390381\n",
            "Epoch:  1\n",
            "Loss:  1.5917896\n",
            "time:  1.0491154193878174\n",
            "Epoch:  1\n",
            "Loss:  1.9175698\n",
            "time:  0.6375405788421631\n",
            "Epoch:  2\n",
            "Loss:  1.5592314\n",
            "time:  0.8358595371246338\n",
            "Epoch:  2\n",
            "Loss:  1.6674801\n",
            "time:  0.634568452835083\n",
            "Epoch:  2\n",
            "Loss:  1.6842817\n",
            "time:  0.64013671875\n",
            "Epoch:  2\n",
            "Loss:  1.7718469\n",
            "time:  0.8423845767974854\n",
            "Epoch:  2\n",
            "Loss:  1.6411729\n",
            "time:  0.6428947448730469\n",
            "Epoch:  2\n",
            "Loss:  1.6962112\n",
            "time:  0.646188497543335\n",
            "Epoch:  2\n",
            "Loss:  1.7935053\n",
            "time:  0.6767301559448242\n",
            "Epoch:  2\n",
            "Loss:  1.7426467\n",
            "time:  0.6427583694458008\n",
            "Epoch:  2\n",
            "Loss:  1.7357162\n",
            "time:  0.8298571109771729\n",
            "Epoch:  2\n",
            "Loss:  1.5271081\n",
            "time:  0.8554503917694092\n",
            "Epoch:  2\n",
            "Loss:  1.8802177\n",
            "time:  1.0134167671203613\n",
            "Epoch:  2\n",
            "Loss:  1.7641397\n",
            "time:  0.8708229064941406\n",
            "Epoch:  2\n",
            "Loss:  1.6398138\n",
            "time:  0.8735594749450684\n",
            "Epoch:  2\n",
            "Loss:  1.5359553\n",
            "time:  0.6651852130889893\n",
            "Epoch:  2\n",
            "Loss:  1.6361209\n",
            "time:  0.6493442058563232\n",
            "Epoch:  2\n",
            "Loss:  1.780541\n",
            "time:  0.6426565647125244\n",
            "Epoch:  2\n",
            "Loss:  1.68607\n",
            "time:  1.0625922679901123\n",
            "Epoch:  2\n",
            "Loss:  1.7440788\n",
            "time:  0.6495015621185303\n",
            "Epoch:  2\n",
            "Loss:  1.5360698\n",
            "time:  0.8370299339294434\n",
            "Epoch:  2\n",
            "Loss:  1.597415\n",
            "time:  0.6451900005340576\n",
            "Epoch:  2\n",
            "Loss:  1.6985011\n",
            "time:  0.8316464424133301\n",
            "Epoch:  2\n",
            "Loss:  1.6531781\n",
            "time:  0.655125617980957\n",
            "Epoch:  2\n",
            "Loss:  1.6074106\n",
            "time:  0.8384864330291748\n",
            "Epoch:  2\n",
            "Loss:  1.6704364\n",
            "time:  0.8329694271087646\n",
            "Epoch:  2\n",
            "Loss:  1.7329402\n",
            "time:  0.6618998050689697\n",
            "Epoch:  2\n",
            "Loss:  1.6484604\n",
            "time:  0.8509476184844971\n",
            "Epoch:  2\n",
            "Loss:  1.5812179\n",
            "time:  0.8196148872375488\n",
            "Epoch:  2\n",
            "Loss:  1.6290895\n",
            "time:  0.8651227951049805\n",
            "Epoch:  2\n",
            "Loss:  1.6617011\n",
            "time:  0.6148040294647217\n",
            "Epoch:  3\n",
            "Loss:  1.4491735\n",
            "time:  0.637566328048706\n",
            "Epoch:  3\n",
            "Loss:  1.4924664\n",
            "time:  0.6240127086639404\n",
            "Epoch:  3\n",
            "Loss:  1.515128\n",
            "time:  0.8373777866363525\n",
            "Epoch:  3\n",
            "Loss:  1.6164778\n",
            "time:  0.8544530868530273\n",
            "Epoch:  3\n",
            "Loss:  1.575513\n",
            "time:  0.8501420021057129\n",
            "Epoch:  3\n",
            "Loss:  1.4128839\n",
            "time:  0.7813692092895508\n",
            "Epoch:  3\n",
            "Loss:  1.4285307\n",
            "time:  0.6454761028289795\n",
            "Epoch:  3\n",
            "Loss:  1.5676712\n",
            "time:  0.8363749980926514\n",
            "Epoch:  3\n",
            "Loss:  1.614173\n",
            "time:  0.6313309669494629\n",
            "Epoch:  3\n",
            "Loss:  1.6261706\n",
            "time:  0.6162431240081787\n",
            "Epoch:  3\n",
            "Loss:  1.6953129\n",
            "time:  0.6179463863372803\n",
            "Epoch:  3\n",
            "Loss:  1.5796365\n",
            "time:  0.8501331806182861\n",
            "Epoch:  3\n",
            "Loss:  1.6427068\n",
            "time:  0.6421937942504883\n",
            "Epoch:  3\n",
            "Loss:  1.5338745\n",
            "time:  0.6446096897125244\n",
            "Epoch:  3\n",
            "Loss:  1.6065879\n",
            "time:  0.647674560546875\n",
            "Epoch:  3\n",
            "Loss:  1.5486817\n",
            "time:  0.6220686435699463\n",
            "Epoch:  3\n",
            "Loss:  1.4492539\n",
            "time:  0.6382076740264893\n",
            "Epoch:  3\n",
            "Loss:  1.5663619\n",
            "time:  1.0274760723114014\n",
            "Epoch:  3\n",
            "Loss:  1.6562771\n",
            "time:  0.6347000598907471\n",
            "Epoch:  3\n",
            "Loss:  1.690527\n",
            "time:  1.2473077774047852\n",
            "Epoch:  3\n",
            "Loss:  1.3573675\n",
            "time:  0.836557149887085\n",
            "Epoch:  3\n",
            "Loss:  1.5303636\n",
            "time:  0.8519887924194336\n",
            "Epoch:  3\n",
            "Loss:  1.5428141\n",
            "time:  0.8405847549438477\n",
            "Epoch:  3\n",
            "Loss:  1.6032737\n",
            "time:  0.6319453716278076\n",
            "Epoch:  3\n",
            "Loss:  1.5001912\n",
            "time:  0.8367574214935303\n",
            "Epoch:  3\n",
            "Loss:  1.7182307\n",
            "time:  0.6484065055847168\n",
            "Epoch:  3\n",
            "Loss:  1.495152\n",
            "time:  0.6416125297546387\n",
            "Epoch:  3\n",
            "Loss:  1.5751551\n",
            "time:  0.6383404731750488\n",
            "Epoch:  3\n",
            "Loss:  1.5377897\n",
            "time:  0.6705787181854248\n",
            "Epoch:  4\n",
            "Loss:  1.4043683\n",
            "time:  0.6306126117706299\n",
            "Epoch:  4\n",
            "Loss:  1.3530312\n",
            "time:  0.8371844291687012\n",
            "Epoch:  4\n",
            "Loss:  1.3907083\n",
            "time:  0.635758638381958\n",
            "Epoch:  4\n",
            "Loss:  1.4731385\n",
            "time:  0.8171396255493164\n",
            "Epoch:  4\n",
            "Loss:  1.5696899\n",
            "time:  0.6396758556365967\n",
            "Epoch:  4\n",
            "Loss:  1.5082121\n",
            "time:  0.8225550651550293\n",
            "Epoch:  4\n",
            "Loss:  1.4367381\n",
            "time:  0.827207088470459\n",
            "Epoch:  4\n",
            "Loss:  1.4261061\n",
            "time:  0.8228545188903809\n",
            "Epoch:  4\n",
            "Loss:  1.2886697\n",
            "time:  0.8262863159179688\n",
            "Epoch:  4\n",
            "Loss:  1.4442015\n",
            "time:  0.6500744819641113\n",
            "Epoch:  4\n",
            "Loss:  1.44499\n",
            "time:  0.648101806640625\n",
            "Epoch:  4\n",
            "Loss:  1.453959\n",
            "time:  0.823265790939331\n",
            "Epoch:  4\n",
            "Loss:  1.4789542\n",
            "time:  0.6351006031036377\n",
            "Epoch:  4\n",
            "Loss:  1.475974\n",
            "time:  0.6485023498535156\n",
            "Epoch:  4\n",
            "Loss:  1.5334882\n",
            "time:  0.8255467414855957\n",
            "Epoch:  4\n",
            "Loss:  1.5078588\n",
            "time:  0.6486752033233643\n",
            "Epoch:  4\n",
            "Loss:  1.4968547\n",
            "time:  0.6319122314453125\n",
            "Epoch:  4\n",
            "Loss:  1.4417976\n",
            "time:  0.8304734230041504\n",
            "Epoch:  4\n",
            "Loss:  1.415507\n",
            "time:  0.6297645568847656\n",
            "Epoch:  4\n",
            "Loss:  1.3861594\n",
            "time:  0.6425600051879883\n",
            "Epoch:  4\n",
            "Loss:  1.601922\n",
            "time:  0.615300178527832\n",
            "Epoch:  4\n",
            "Loss:  1.3990947\n",
            "time:  0.6276073455810547\n",
            "Epoch:  4\n",
            "Loss:  1.4532557\n",
            "time:  0.6323251724243164\n",
            "Epoch:  4\n",
            "Loss:  1.580533\n",
            "time:  0.8037264347076416\n",
            "Epoch:  4\n",
            "Loss:  1.4855013\n",
            "time:  0.8551836013793945\n",
            "Epoch:  4\n",
            "Loss:  1.5943114\n",
            "time:  0.876922607421875\n",
            "Epoch:  4\n",
            "Loss:  1.5149783\n",
            "time:  0.8428525924682617\n",
            "Epoch:  4\n",
            "Loss:  1.6371723\n",
            "time:  1.0302326679229736\n",
            "Epoch:  4\n",
            "Loss:  1.6312622\n",
            "time:  0.6382596492767334\n",
            "Epoch:  5\n",
            "Loss:  1.3081965\n",
            "time:  1.024796485900879\n",
            "Epoch:  5\n",
            "Loss:  1.2598664\n",
            "time:  0.6578066349029541\n",
            "Epoch:  5\n",
            "Loss:  1.3271805\n",
            "time:  0.6412415504455566\n",
            "Epoch:  5\n",
            "Loss:  1.4239565\n",
            "time:  0.6468374729156494\n",
            "Epoch:  5\n",
            "Loss:  1.4538702\n",
            "time:  0.6407406330108643\n",
            "Epoch:  5\n",
            "Loss:  1.4574208\n",
            "time:  0.6379616260528564\n",
            "Epoch:  5\n",
            "Loss:  1.3910012\n",
            "time:  0.6998245716094971\n",
            "Epoch:  5\n",
            "Loss:  1.418758\n",
            "time:  0.6495237350463867\n",
            "Epoch:  5\n",
            "Loss:  1.4985694\n",
            "time:  0.631371259689331\n",
            "Epoch:  5\n",
            "Loss:  1.3601643\n",
            "time:  0.6611764430999756\n",
            "Epoch:  5\n",
            "Loss:  1.4990414\n",
            "time:  2.467191457748413\n",
            "Epoch:  5\n",
            "Loss:  1.5635185\n",
            "time:  1.9358749389648438\n",
            "Epoch:  5\n",
            "Loss:  1.4462098\n",
            "time:  0.6525673866271973\n",
            "Epoch:  5\n",
            "Loss:  1.3733963\n",
            "time:  0.8126814365386963\n",
            "Epoch:  5\n",
            "Loss:  1.20026\n",
            "time:  0.6237342357635498\n",
            "Epoch:  5\n",
            "Loss:  1.3698581\n",
            "time:  0.6252691745758057\n",
            "Epoch:  5\n",
            "Loss:  1.4992929\n",
            "time:  0.6373672485351562\n",
            "Epoch:  5\n",
            "Loss:  1.3326136\n",
            "time:  0.6488006114959717\n",
            "Epoch:  5\n",
            "Loss:  1.6091409\n",
            "time:  0.6208221912384033\n",
            "Epoch:  5\n",
            "Loss:  1.5948416\n",
            "time:  0.8529095649719238\n",
            "Epoch:  5\n",
            "Loss:  1.230297\n",
            "time:  0.857168436050415\n",
            "Epoch:  5\n",
            "Loss:  1.3510696\n",
            "time:  0.6503734588623047\n",
            "Epoch:  5\n",
            "Loss:  1.2410067\n",
            "time:  0.6270349025726318\n",
            "Epoch:  5\n",
            "Loss:  1.3980289\n",
            "time:  0.7133152484893799\n",
            "Epoch:  5\n",
            "Loss:  1.5131949\n",
            "time:  0.838569164276123\n",
            "Epoch:  5\n",
            "Loss:  1.3220111\n",
            "time:  1.3997924327850342\n",
            "Epoch:  5\n",
            "Loss:  1.537903\n",
            "time:  0.65635085105896\n",
            "Epoch:  5\n",
            "Loss:  1.4582742\n",
            "time:  0.6340134143829346\n",
            "Epoch:  5\n",
            "Loss:  1.3521308\n",
            "time:  0.8520264625549316\n",
            "Epoch:  6\n",
            "Loss:  1.1302415\n",
            "time:  0.6273543834686279\n",
            "Epoch:  6\n",
            "Loss:  1.2328688\n",
            "time:  0.8543150424957275\n",
            "Epoch:  6\n",
            "Loss:  1.3880931\n",
            "time:  0.8353531360626221\n",
            "Epoch:  6\n",
            "Loss:  1.3605613\n",
            "time:  0.6300160884857178\n",
            "Epoch:  6\n",
            "Loss:  1.3307215\n",
            "time:  0.6495521068572998\n",
            "Epoch:  6\n",
            "Loss:  1.4607882\n",
            "time:  0.8622629642486572\n",
            "Epoch:  6\n",
            "Loss:  1.513265\n",
            "time:  0.640474796295166\n",
            "Epoch:  6\n",
            "Loss:  1.3768425\n",
            "time:  0.6349313259124756\n",
            "Epoch:  6\n",
            "Loss:  1.3854061\n",
            "time:  0.6289181709289551\n",
            "Epoch:  6\n",
            "Loss:  1.3556103\n",
            "time:  0.6308043003082275\n",
            "Epoch:  6\n",
            "Loss:  1.2455679\n",
            "time:  0.8133881092071533\n",
            "Epoch:  6\n",
            "Loss:  1.3847141\n",
            "time:  0.8418090343475342\n",
            "Epoch:  6\n",
            "Loss:  1.3314148\n",
            "time:  0.8451368808746338\n",
            "Epoch:  6\n",
            "Loss:  1.4039527\n",
            "time:  0.6533422470092773\n",
            "Epoch:  6\n",
            "Loss:  1.2333932\n",
            "time:  0.6367621421813965\n",
            "Epoch:  6\n",
            "Loss:  1.4743994\n",
            "time:  0.8504927158355713\n",
            "Epoch:  6\n",
            "Loss:  1.2187473\n",
            "time:  0.851449728012085\n",
            "Epoch:  6\n",
            "Loss:  1.3337942\n",
            "time:  0.6319568157196045\n",
            "Epoch:  6\n",
            "Loss:  1.340068\n",
            "time:  0.6409602165222168\n",
            "Epoch:  6\n",
            "Loss:  1.289888\n",
            "time:  0.8349816799163818\n",
            "Epoch:  6\n",
            "Loss:  1.5169393\n",
            "time:  0.6372616291046143\n",
            "Epoch:  6\n",
            "Loss:  1.4484512\n",
            "time:  0.6263039112091064\n",
            "Epoch:  6\n",
            "Loss:  1.3943592\n",
            "time:  0.6321530342102051\n",
            "Epoch:  6\n",
            "Loss:  1.4004703\n",
            "time:  0.6410059928894043\n",
            "Epoch:  6\n",
            "Loss:  1.2951847\n",
            "time:  0.6353938579559326\n",
            "Epoch:  6\n",
            "Loss:  1.2644166\n",
            "time:  0.635667085647583\n",
            "Epoch:  6\n",
            "Loss:  1.3894691\n",
            "time:  0.6304888725280762\n",
            "Epoch:  6\n",
            "Loss:  1.1787828\n",
            "time:  0.777972936630249\n",
            "Epoch:  6\n",
            "Loss:  1.372086\n",
            "time:  0.6561837196350098\n",
            "Epoch:  7\n",
            "Loss:  1.233013\n",
            "time:  0.6467642784118652\n",
            "Epoch:  7\n",
            "Loss:  1.3623177\n",
            "time:  0.6321420669555664\n",
            "Epoch:  7\n",
            "Loss:  1.3650261\n",
            "time:  0.8269007205963135\n",
            "Epoch:  7\n",
            "Loss:  1.2688843\n",
            "time:  0.8456637859344482\n",
            "Epoch:  7\n",
            "Loss:  1.2784152\n",
            "time:  0.8101053237915039\n",
            "Epoch:  7\n",
            "Loss:  1.2803522\n",
            "time:  0.6443476676940918\n",
            "Epoch:  7\n",
            "Loss:  1.22481\n",
            "time:  0.6737723350524902\n",
            "Epoch:  7\n",
            "Loss:  1.2674626\n",
            "time:  0.6501672267913818\n",
            "Epoch:  7\n",
            "Loss:  1.3523594\n",
            "time:  0.6250011920928955\n",
            "Epoch:  7\n",
            "Loss:  1.3584027\n",
            "time:  0.644991397857666\n",
            "Epoch:  7\n",
            "Loss:  1.2270597\n",
            "time:  0.6250600814819336\n",
            "Epoch:  7\n",
            "Loss:  1.2365961\n",
            "time:  0.6239094734191895\n",
            "Epoch:  7\n",
            "Loss:  1.3135585\n",
            "time:  0.62929368019104\n",
            "Epoch:  7\n",
            "Loss:  1.2484143\n",
            "time:  0.6251230239868164\n",
            "Epoch:  7\n",
            "Loss:  1.3074986\n",
            "time:  0.62550950050354\n",
            "Epoch:  7\n",
            "Loss:  1.3127898\n",
            "time:  0.6554644107818604\n",
            "Epoch:  7\n",
            "Loss:  1.1293446\n",
            "time:  0.6394636631011963\n",
            "Epoch:  7\n",
            "Loss:  1.3291584\n",
            "time:  0.635028600692749\n",
            "Epoch:  7\n",
            "Loss:  1.252809\n",
            "time:  0.6271886825561523\n",
            "Epoch:  7\n",
            "Loss:  1.3928126\n",
            "time:  0.6226205825805664\n",
            "Epoch:  7\n",
            "Loss:  1.3421837\n",
            "time:  0.7892534732818604\n",
            "Epoch:  7\n",
            "Loss:  1.1961752\n",
            "time:  0.8275132179260254\n",
            "Epoch:  7\n",
            "Loss:  1.2607542\n",
            "time:  0.8771317005157471\n",
            "Epoch:  7\n",
            "Loss:  1.3838987\n",
            "time:  0.6342935562133789\n",
            "Epoch:  7\n",
            "Loss:  1.1946497\n",
            "time:  0.6287562847137451\n",
            "Epoch:  7\n",
            "Loss:  1.3554788\n",
            "time:  0.6309077739715576\n",
            "Epoch:  7\n",
            "Loss:  1.3554502\n",
            "time:  0.6297106742858887\n",
            "Epoch:  7\n",
            "Loss:  1.3026196\n",
            "time:  0.6268036365509033\n",
            "Epoch:  7\n",
            "Loss:  1.2489649\n",
            "time:  0.6356196403503418\n",
            "Epoch:  8\n",
            "Loss:  1.1133928\n",
            "time:  0.6416645050048828\n",
            "Epoch:  8\n",
            "Loss:  1.244729\n",
            "time:  0.6236021518707275\n",
            "Epoch:  8\n",
            "Loss:  1.1951474\n",
            "time:  0.6275126934051514\n",
            "Epoch:  8\n",
            "Loss:  1.174981\n",
            "time:  0.6292243003845215\n",
            "Epoch:  8\n",
            "Loss:  1.1078585\n",
            "time:  0.6389665603637695\n",
            "Epoch:  8\n",
            "Loss:  1.1527866\n",
            "time:  0.6534709930419922\n",
            "Epoch:  8\n",
            "Loss:  1.3394529\n",
            "time:  0.6374149322509766\n",
            "Epoch:  8\n",
            "Loss:  1.2639037\n",
            "time:  0.6553258895874023\n",
            "Epoch:  8\n",
            "Loss:  1.3155931\n",
            "time:  1.0160558223724365\n",
            "Epoch:  8\n",
            "Loss:  1.2976972\n",
            "time:  0.8796887397766113\n",
            "Epoch:  8\n",
            "Loss:  1.1478318\n",
            "time:  0.8740096092224121\n",
            "Epoch:  8\n",
            "Loss:  1.2406033\n",
            "time:  0.6198086738586426\n",
            "Epoch:  8\n",
            "Loss:  1.2072557\n",
            "time:  0.6104321479797363\n",
            "Epoch:  8\n",
            "Loss:  1.164683\n",
            "time:  0.6371166706085205\n",
            "Epoch:  8\n",
            "Loss:  1.3243984\n",
            "time:  0.6433582305908203\n",
            "Epoch:  8\n",
            "Loss:  1.3779644\n",
            "time:  0.6269176006317139\n",
            "Epoch:  8\n",
            "Loss:  1.3105069\n",
            "time:  0.633723258972168\n",
            "Epoch:  8\n",
            "Loss:  1.3383223\n",
            "time:  0.6294083595275879\n",
            "Epoch:  8\n",
            "Loss:  1.2452334\n",
            "time:  0.6334578990936279\n",
            "Epoch:  8\n",
            "Loss:  1.2134963\n",
            "time:  0.626220703125\n",
            "Epoch:  8\n",
            "Loss:  1.1733166\n",
            "time:  0.6329903602600098\n",
            "Epoch:  8\n",
            "Loss:  1.2278693\n",
            "time:  0.6270148754119873\n",
            "Epoch:  8\n",
            "Loss:  1.293476\n",
            "time:  0.6330208778381348\n",
            "Epoch:  8\n",
            "Loss:  1.2873963\n",
            "time:  0.8324418067932129\n",
            "Epoch:  8\n",
            "Loss:  1.0818299\n",
            "time:  0.6566958427429199\n",
            "Epoch:  8\n",
            "Loss:  1.2951342\n",
            "time:  0.6258180141448975\n",
            "Epoch:  8\n",
            "Loss:  1.2837368\n",
            "time:  0.7879281044006348\n",
            "Epoch:  8\n",
            "Loss:  1.2965205\n",
            "time:  0.8610446453094482\n",
            "Epoch:  8\n",
            "Loss:  1.2773628\n",
            "time:  0.6451692581176758\n",
            "Epoch:  9\n",
            "Loss:  1.2377521\n",
            "time:  0.6439073085784912\n",
            "Epoch:  9\n",
            "Loss:  1.1232046\n",
            "time:  0.6454780101776123\n",
            "Epoch:  9\n",
            "Loss:  1.2230417\n",
            "time:  0.6386888027191162\n",
            "Epoch:  9\n",
            "Loss:  1.1483928\n",
            "time:  0.8514432907104492\n",
            "Epoch:  9\n",
            "Loss:  1.0958048\n",
            "time:  0.6523661613464355\n",
            "Epoch:  9\n",
            "Loss:  1.1971337\n",
            "time:  0.6371030807495117\n",
            "Epoch:  9\n",
            "Loss:  1.1206298\n",
            "time:  0.6386415958404541\n",
            "Epoch:  9\n",
            "Loss:  1.2905205\n",
            "time:  0.6431636810302734\n",
            "Epoch:  9\n",
            "Loss:  1.1134604\n",
            "time:  0.6228001117706299\n",
            "Epoch:  9\n",
            "Loss:  1.1910484\n",
            "time:  0.6501412391662598\n",
            "Epoch:  9\n",
            "Loss:  1.0857415\n",
            "time:  0.6358098983764648\n",
            "Epoch:  9\n",
            "Loss:  1.2537766\n",
            "time:  0.6240484714508057\n",
            "Epoch:  9\n",
            "Loss:  1.3001287\n",
            "time:  0.6379125118255615\n",
            "Epoch:  9\n",
            "Loss:  1.2365215\n",
            "time:  0.7656855583190918\n",
            "Epoch:  9\n",
            "Loss:  1.1498936\n",
            "time:  0.8528413772583008\n",
            "Epoch:  9\n",
            "Loss:  1.3339874\n",
            "time:  0.8723649978637695\n",
            "Epoch:  9\n",
            "Loss:  1.075542\n",
            "time:  0.638756513595581\n",
            "Epoch:  9\n",
            "Loss:  1.2138379\n",
            "time:  0.626920223236084\n",
            "Epoch:  9\n",
            "Loss:  1.2770296\n",
            "time:  0.6271688938140869\n",
            "Epoch:  9\n",
            "Loss:  1.0755805\n",
            "time:  0.8652472496032715\n",
            "Epoch:  9\n",
            "Loss:  1.122917\n",
            "time:  0.6342692375183105\n",
            "Epoch:  9\n",
            "Loss:  1.1745915\n",
            "time:  0.8435194492340088\n",
            "Epoch:  9\n",
            "Loss:  1.331839\n",
            "time:  0.7951364517211914\n",
            "Epoch:  9\n",
            "Loss:  1.2428873\n",
            "time:  0.6443066596984863\n",
            "Epoch:  9\n",
            "Loss:  1.2845007\n",
            "time:  0.6476671695709229\n",
            "Epoch:  9\n",
            "Loss:  1.1796038\n",
            "time:  0.6383898258209229\n",
            "Epoch:  9\n",
            "Loss:  1.2387149\n",
            "time:  0.6386234760284424\n",
            "Epoch:  9\n",
            "Loss:  1.2611881\n",
            "time:  0.6256520748138428\n",
            "Epoch:  9\n",
            "Loss:  1.0713531\n"
          ]
        }
      ],
      "source": [
        "EPOCH = 100\n",
        "for epoch in range(EPOCH):\n",
        "    enc_hidden = encoder.initilize_hidden_state()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        start = time.time()\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        end = time.time()\n",
        "        print('time: ', end - start)\n",
        "        print('Epoch: ', epoch)\n",
        "        print('Loss: ', batch_loss.numpy())\n",
        "    checkpoint.save(file_prefix='test1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qkn4fyqIwQDF"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    sentence = preprocess_senetence(sentence)\n",
        "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<strat>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0zD1T98IwQDI",
        "outputId": "c4a9b4e5-cc3b-4c5c-9ce2-2358a999a1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f010d540310>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(''))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lE_OLnWAI-2",
        "outputId": "cb1376d0-2e9a-48f9-9400-1b039a5af754"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import colorama \n",
        "colorama.init()\n",
        "from colorama import Fore, Style, Back\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
        "    inp = input()\n",
        "    if inp.lower() == \"quit\":\n",
        "        break\n",
        "    result = evaluate(inp)    \n",
        "    print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL ,result[0])\n",
        "\n",
        "      # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))\n",
        "\n",
        "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W9c-k3M_gQ6",
        "outputId": "58ffd59f-e2a9-47c0-fecd-084dd1f719e4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hi\n",
            "ChatBot: yes , but i enjoy the street , but i enjoy the street , but i enjoy the street , but i enjoy the \n",
            "User: quit\n",
            "Start messaging with the bot (type quit to stop)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsm21zaWAeZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}